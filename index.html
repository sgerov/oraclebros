<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="robots" content="index, follow">
		<title>Oraclebros.com</title>
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300italic,700,700italic">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/3.0.3/normalize.css">
		<link rel="stylesheet" href="style.css">
		<link rel="icon" href="https://oraclebros.com/favicon-96x96.png">
		<link rel="prefetch" type="application/l10n" href="translations.ini" />
	</head>
	<body>

		<main class="wrapper">

			<nav class="navigation">
				<section class="container" style="display: flex; justify-content: space-between;">

					<a class="navigation-title" href="">
						<img class="img" src="https://oraclebros.com/agent-smith.png" height="30">
						<h1 class="title">Oraclebros.com</h1>
					</a>
					
					<span>
						<a class="" href="https://explorer.radixdlt.com/#/validators/rv1qg0qqwmjun43knkyyhh5kavhljy6ml5ap5hnzr2ahqwyt7n0rqq56pehq5n">
							<img class="img" src="https://s2.coinmarketcap.com/static/img/coins/64x64/7692.png" height="30">
							<h1 class="title">Radix Profile</h1>
						</a>
						<a class="" href="https://discord.gg/NH47Zxc8Re">
							<img class="img" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQo4yQ5O1kv2Uv1Y45cZQlZpvhgmx1oWW8nqg&usqp=CAU">
							<h1 class="title">Get in touch</h1>
						</a>
					</span>
				</section>
			</nav>

			<section class="container" id="main">
				<div class="panel" style="background-color: #1192fc;">
					Stake address: rv1qg0qqwmjun43knkyyhh5kavhljy6ml5ap5hnzr2ahqwyt7n0rqq56pehq5n
				</div>
				
					<div class="panel operational" data-l10n-id="systems-operational">
						All Systems Operational
					</div>
				

				<h4 data-l10n-id="systems">Systems</h4>
				<ul class="systems">
					
					<li>
						Monitoring system <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Olympia Backup Node <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Olympia Validator <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Stokenet <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
				</ul>

				<h4 data-l10n-id="incidents">Incidents</h4>
				
					
					<div class="incident">
						<span class="date moment">2021-12-10 08:13:00 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Down to 99.2%</span>
						<p>During the night we got some more missed proposals and didn't recover our uptime. We are looking at our options right now</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:32:28 UTC</span></em></p>
							<p>Switched to Backup node while we are cleaning up the address book of the main node. We suspect that our address book is way out of sync due to peer connection timeouts in our logs.</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:41:12 UTC</span></em></p>
							<p>Cleaned up Main node address book and switched back to it. Backup node up and running again as well</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:43:42 UTC</span></em></p>
							<p>Node is stable again, we operated during 6 minutes with our backup node while we cleaned up the address of the main one.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145600999-3e0dd3a1-e72d-4094-9c8d-cefab3f000b2.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-10 15:44:56 UTC</span></em></p>
							<p>Our alerting was quite unstable lately, we expect the issue to be gone after the Address book cleanup which removed around 530 addresses that aren't in use anymore. You can see our alerts in the last 7 days below:</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145601245-696a6cb2-c049-4dbc-941d-4453b637de6e.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-10 15:46:25 UTC</span></em></p>
							<p>No downtime during the nodes switches within our Kubernetes cluster now that our terraform resources are back up and running and we can do the switches as usual. We will be monitoring closely the situation</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:51:58 UTC</span></em></p>
							<p>We saw no network issues nor CPU/RAM/Provider downtime during the high alerting period.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145602067-b2100b00-75e8-44c0-b1e8-8c6b2490417b.png" alt="image" />
<img src="https://user-images.githubusercontent.com/1419573/145602258-9ea165de-1c15-4853-b6df-e069a45f8c5a.png" alt="image" />
<img src="https://user-images.githubusercontent.com/1419573/145602289-337de5ab-a81f-43f0-a82b-edcbe96985f7.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-11 14:40:30 UTC</span></em></p>
							<p>Our reverse proxy (NGINX) had lost it's connection to our main node and was not allowing connection which caused timeouts for our node. The issue has been fixed and we added further alarming to detect if such case happened again. We expect our uptime to recover from now on</p>

						
							<p><em>Update <span class="moment">2021-12-18 10:11:35 UTC</span></em></p>
							<p>Took us some time till the uptime recovered but we are back to normal now</p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-12-07 18:34:26 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Monitoring system</span>
						
						<hr/>

						<span class="title">Terraform resources</span>
						<p>We lost our terraform state file due to a hardware switch, we need to recover our resources through <code>import</code>. We still can manage our infrastructure in the meantime</p>

						
							<p><em>Update <span class="moment">2021-12-07 18:44:26 UTC</span></em></p>
							<p>For the sake of clarity: monitoring is up and running, so is our infrastructure. It's our infrastructure as code pipeline that has issues, which can still be managed by hand if needed.</p>

						
							<p><em>Update <span class="moment">2021-12-09 11:43:50 UTC</span></em></p>
							<p>Resources of our backup node and its configs have been successfully recovered</p>

						
							<p><em>Update <span class="moment">2021-12-09 11:53:06 UTC</span></em></p>
							<p>Our backup node monitoring system had a 5 minutes downtime during the transition. This isn't necessary so we are looking how to ensure this won't happen for our main node.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145391663-1109886d-9f07-421d-93be-7db2bce90220.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-09 13:19:41 UTC</span></em></p>
							<p>All resources are back to normal</p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-12-07 16:22:52 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Uptime down to 99.81%</span>
						<p>We have experienced some missed proposals, the team is looking into it</p>

						
							<p><em>Update <span class="moment">2021-12-07 17:19:36 UTC</span></em></p>
							<p>We didn't switch immediately to <code>1.0.3</code> as it was released since we were happy with our UPTIME and weren't too worried about the memory leak since we have plenty of RAM. We are planning to do the switch currently though</p>

						
							<p><em>Update <span class="moment">2021-12-07 18:33:00 UTC</span></em></p>
							<p>Since some nodes are experiencing issues in the network now (AWS issues) we will wait for our upgrade. Also we would prefer to get our UPTIME higher before the switch since we won't be unregistering our node but aiming for 0 downtime.</p>

						
							<p><em>Update <span class="moment">2021-12-09 12:33:06 UTC</span></em></p>
							<p>Switching to our backup node to perform maintenance on our main hardware</p>

						
							<p><em>Update <span class="moment">2021-12-09 12:45:42 UTC</span></em></p>
							<p>Successfully switched to backup node, starting maintenance</p>

						
							<p><em>Update <span class="moment">2021-12-09 13:15:00 UTC</span></em></p>
							<p>Maintenance finished and switched back to our most powerful hardware</p>

						
							<p><em>Update <span class="moment">2021-12-09 13:22:06 UTC</span></em></p>
							<p>Human error: we switched too quickly (validator desynced while we performed maintenance) from the Backup back to the Main node and missed a few proposals during the re-syncing time (a couple of minutes):</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145403949-4ff1bb94-e607-4a75-8d45-7f0f116ec06b.png" alt="image" /></p>

<p>This brought us down to 99.41% but we don't expect further failures. We are keeping an eye though</p>

						
					</div>
					
				
			</section>


			<footer class="footer">
				<section class="container">
					<hr/>
					<p>If you experience any issue please contact us on Discord: https://discord.gg/NH47Zxc8Re</p>
				</section>
			</footer>

		</main>
		<script src="statuspage.js" type="text/javascript"></script>
	</body>
</html>