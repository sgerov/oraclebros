<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="robots" content="index, follow">
		<title>Oraclebros.com</title>
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300italic,700,700italic">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/3.0.3/normalize.css">
		<link rel="stylesheet" href="style.css">
		<link rel="icon" href="https://oraclebros.com/favicon-96x96.png">
		<link rel="prefetch" type="application/l10n" href="translations.ini" />
	</head>
	<body>

		<main class="wrapper">

			<nav class="navigation">
				<section class="container" style="display: flex; justify-content: space-between;">

					<a class="navigation-title" href="">
						<img class="img" src="https://oraclebros.com/agent-smith.png" height="30">
						<h1 class="title">Oraclebros.com</h1>
					</a>
					
					<span>
						<a class="" href="https://explorer.radixdlt.com/#/validators/rv1qg0qqwmjun43knkyyhh5kavhljy6ml5ap5hnzr2ahqwyt7n0rqq56pehq5n">
							<img class="img" src="https://s2.coinmarketcap.com/static/img/coins/64x64/7692.png" height="30">
							<h1 class="title">Radix Profile</h1>
						</a>
						<a class="" href="https://discord.gg/NH47Zxc8Re">
							<img class="img" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQo4yQ5O1kv2Uv1Y45cZQlZpvhgmx1oWW8nqg&usqp=CAU">
							<h1 class="title">Get in touch</h1>
						</a>
					</span>
				</section>
			</nav>

			<section class="container" id="main">
				<div class="panel" style="background-color: #1192fc;">
					Stake address: rv1qg0qqwmjun43knkyyhh5kavhljy6ml5ap5hnzr2ahqwyt7n0rqq56pehq5n
				</div>
				
					
					<div class="panel degraded performance">
						<span data-l10n-id="Degraded-performance">Degraded performance</span> <span data-l10n-id="on">on</span> Radix Olympia Validator.
					</div>
					
				

				<h4 data-l10n-id="systems">Systems</h4>
				<ul class="systems">
					
					<li>
						Monitoring system <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Olympia Backup Node <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Olympia Validator <span class="status degraded performance" data-l10n-id="degraded-performance">degraded performance</span>
					</li>
					
					<li>
						Radix Stokenet <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
				</ul>

				<h4 data-l10n-id="incidents">Incidents</h4>
				
					
					<div class="incident">
						<span class="date moment">2021-12-10 08:13:00 UTC</span>

						
							<span class="label degraded performance float-right" data-l10n-id="degraded-performance">degraded performance</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Down to 99.2%</span>
						<p>During the night we got some more missed proposals and didn't recover our uptime. We are looking at our options right now</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:32:28 UTC</span></em></p>
							<p>Switched to Backup node while we are cleaning up the address book of the main node. We suspect that our address book is way out of sync due to peer connection timeouts in our logs.</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:41:12 UTC</span></em></p>
							<p>Cleaned up Main node address book and switched back to it. Backup node up and running again as well</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:43:42 UTC</span></em></p>
							<p>Node is stable again, we operated during 6 minutes with our backup node while we cleaned up the address of the main one.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145600999-3e0dd3a1-e72d-4094-9c8d-cefab3f000b2.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-10 15:44:56 UTC</span></em></p>
							<p>Our alerting was quite unstable lately, we expect the issue to be gone after the Address book cleanup which removed around 530 addresses that aren't in use anymore. You can see our alerts in the last 7 days below:</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145601245-696a6cb2-c049-4dbc-941d-4453b637de6e.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-10 15:46:25 UTC</span></em></p>
							<p>No downtime during the nodes switches within our Kubernetes cluster now that our terraform resources are back up and running and we can do the switches as usual. We will be monitoring closely the situation</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:51:58 UTC</span></em></p>
							<p>We saw no network issues nor CPU/RAM/Provider downtime during the high alerting period.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145602067-b2100b00-75e8-44c0-b1e8-8c6b2490417b.png" alt="image" />
<img src="https://user-images.githubusercontent.com/1419573/145602258-9ea165de-1c15-4853-b6df-e069a45f8c5a.png" alt="image" />
<img src="https://user-images.githubusercontent.com/1419573/145602289-337de5ab-a81f-43f0-a82b-edcbe96985f7.png" alt="image" /></p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-12-07 18:34:26 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Monitoring system</span>
						
						<hr/>

						<span class="title">Terraform resources</span>
						<p>We lost our terraform state file due to a hardware switch, we need to recover our resources through <code>import</code>. We still can manage our infrastructure in the meantime</p>

						
							<p><em>Update <span class="moment">2021-12-07 18:44:26 UTC</span></em></p>
							<p>For the sake of clarity: monitoring is up and running, so is our infrastructure. It's our infrastructure as code pipeline that has issues, which can still be managed by hand if needed.</p>

						
							<p><em>Update <span class="moment">2021-12-09 11:43:50 UTC</span></em></p>
							<p>Resources of our backup node and its configs have been successfully recovered</p>

						
							<p><em>Update <span class="moment">2021-12-09 11:53:06 UTC</span></em></p>
							<p>Our backup node monitoring system had a 5 minutes downtime during the transition. This isn't necessary so we are looking how to ensure this won't happen for our main node.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145391663-1109886d-9f07-421d-93be-7db2bce90220.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-09 13:19:41 UTC</span></em></p>
							<p>All resources are back to normal</p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-12-07 16:22:52 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Uptime down to 99.81%</span>
						<p>We have experienced some missed proposals, the team is looking into it</p>

						
							<p><em>Update <span class="moment">2021-12-07 17:19:36 UTC</span></em></p>
							<p>We didn't switch immediately to <code>1.0.3</code> as it was released since we were happy with our UPTIME and weren't too worried about the memory leak since we have plenty of RAM. We are planning to do the switch currently though</p>

						
							<p><em>Update <span class="moment">2021-12-07 18:33:00 UTC</span></em></p>
							<p>Since some nodes are experiencing issues in the network now (AWS issues) we will wait for our upgrade. Also we would prefer to get our UPTIME higher before the switch since we won't be unregistering our node but aiming for 0 downtime.</p>

						
							<p><em>Update <span class="moment">2021-12-09 12:33:06 UTC</span></em></p>
							<p>Switching to our backup node to perform maintenance on our main hardware</p>

						
							<p><em>Update <span class="moment">2021-12-09 12:45:42 UTC</span></em></p>
							<p>Successfully switched to backup node, starting maintenance</p>

						
							<p><em>Update <span class="moment">2021-12-09 13:15:00 UTC</span></em></p>
							<p>Maintenance finished and switched back to our most powerful hardware</p>

						
							<p><em>Update <span class="moment">2021-12-09 13:22:06 UTC</span></em></p>
							<p>Human error: we switched too quickly (validator desynced while we performed maintenance) from the Backup back to the Main node and missed a few proposals during the re-syncing time (a couple of minutes):</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145403949-4ff1bb94-e607-4a75-8d45-7f0f116ec06b.png" alt="image" /></p>

<p>This brought us down to 99.41% but we don't expect further failures. We are keeping an eye though</p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-10-23 11:49:11 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Backup Node</span>
						
						<hr/>

						<span class="title">Backup node migration</span>
						<p>We've switched where our Backup node was running to a new location so it will be sync-ing for some time. We aren't starting from a DB backup this time but syncing through network since we don't want to affect our Main node.</p>

<p>If an issue with our main node arose during this period we would transfer quickly all data to the backup location and spin it up quickier.</p>

<p>We also took the opportunity to update the backup to <code>1.0.2</code> (main was already migrated)</p>

						
							<p><em>Update <span class="moment">2021-10-23 13:36:40 UTC</span></em></p>
							<p>Sync has completed, the node is ready for a switch if needed</p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-10-23 11:08:19 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Rolling uptime down to 99.9%</span>
						<p>After our infrastructure upgrade we didn't expect missing proposals, having a look on the cause for going down to 99.93%</p>

						
							<p><em>Update <span class="moment">2021-10-24 19:11:01 UTC</span></em></p>
							<p>We identified an issue with our inbound connections on port 30000, we are working on a fix</p>

						
							<p><em>Update <span class="moment">2021-10-24 20:05:45 UTC</span></em></p>
							<p>We fixed a misconfiguration in our NGINX reverse proxy that didn't deliver the connection to the right upstream</p>

						
							<p><em>Update <span class="moment">2021-10-24 20:14:12 UTC</span></em></p>
							<p>The small gap with no data is due to a rollout of the metrics endpoint, not due to actual downtime.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/138611170-c36a633c-61eb-4da7-9634-693e5752771c.png" alt="image" /></p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-09-30 07:57:02 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Rolling uptime down to 99.3%</span>
						<p>The node is stable although we missed some proposals in the last two weeks. The team is having a look</p>

						
							<p><em>Update <span class="moment">2021-09-30 08:29:24 UTC</span></em></p>
							<p>According to the community the network has been clumsy and many validators have been missing proposals, still we are more affected then others according to <a href="https://explorer.radixdlt.com/#/validators">Radix dashboard</a></p>

						
							<p><em>Update <span class="moment">2021-10-01 07:47:29 UTC</span></em></p>
							<p>No clear reason behind the missed proposals. Metrics look good, no particular spikes nor weird behaviour.The Validator has enough resources. We are considering moving our cluster to a different region / provider. No clear motivation to use the Backup Node since both have very similar behaviour</p>

						
							<p><em>Update <span class="moment">2021-10-02 09:35:33 UTC</span></em></p>
							<p>We are currently looking at the following actions:</p>

<ul>
<li>Spin up a dedicated server instead of VPS for our main node</li>
<li>Increase log retention policy since we don't have enough data to look for inconsistencies</li>
<li>Increase monitoring retention policy, we would like to be able to look further in time to be able to detect patterns in a more informed manner</li>
<li>Refine alerting rules</li>
</ul>

<p>We will keep you informed, we aren't rushing the decision since the node is rather stable currently.</p>

						
							<p><em>Update <span class="moment">2021-10-03 16:15:56 UTC</span></em></p>
							<p>We just finalised getting a dedicated server to avoid unknown unknowns when it comes to our Validator performance. The node will be up and running within the next hours and we will proceed with synchronising with Olympia which we expect to take around 6-7 hours before we do the actual switch</p>

						
							<p><em>Update <span class="moment">2021-10-03 20:31:16 UTC</span></em></p>
							<p>We finished migrating our Validator to a new dedicated server</p>

						
							<p><em>Update <span class="moment">2021-10-03 20:43:31 UTC</span></em></p>
							<p>We will be closely monitoring the behaviour of the node with the new servers and maintain this issue open until we see UPTIME is back as it has always been: 100%!</p>

						
							<p><em>Update <span class="moment">2021-10-03 20:50:17 UTC</span></em></p>
							<p>First check on our metrics is looking good:</p>

<p><img src="https://user-images.githubusercontent.com/1419573/135771080-208dfd92-5bc8-419e-965f-aa6d7c7f3b9d.png" alt="image" />
<img src="https://user-images.githubusercontent.com/1419573/135771073-cbf65a6d-2880-4418-b0c5-e2a87abf08de.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-10-04 07:58:15 UTC</span></em></p>
							<p>We were randomising the validator port we were opening for gossiping, we had <code>30498</code> open instead of <code>30000</code>. Hardcoded it to <code>30000</code></p>

						
							<p><em>Update <span class="moment">2021-10-08 12:49:32 UTC</span></em></p>
							<p>We've been very stable for the last week and are back at 100% uptime</p>

						
					</div>
					
				
			</section>


			<footer class="footer">
				<section class="container">
					<hr/>
					<p>If you experience any issue please contact us on Discord: https://discord.gg/NH47Zxc8Re</p>
				</section>
			</footer>

		</main>
		<script src="statuspage.js" type="text/javascript"></script>
	</body>
</html>