<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="robots" content="index, follow">
		<title>Oraclebros.com</title>
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300italic,700,700italic">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/3.0.3/normalize.css">
		<link rel="stylesheet" href="style.css">
		<link rel="icon" href="https://oraclebros.com/favicon-96x96.png">
		<link rel="prefetch" type="application/l10n" href="translations.ini" />
	</head>
	<body>

		<main class="wrapper">

			<nav class="navigation">
				<section class="container" style="display: flex; justify-content: space-between;">

					<a class="navigation-title" href="">
						<img class="img" src="https://oraclebros.com/agent-smith.png" height="30">
						<h1 class="title">Oraclebros.com</h1>
					</a>
					
					<span>
						<a class="" href="https://explorer.radixdlt.com/#/validators/rv1qg0qqwmjun43knkyyhh5kavhljy6ml5ap5hnzr2ahqwyt7n0rqq56pehq5n">
							<img class="img" src="https://s2.coinmarketcap.com/static/img/coins/64x64/7692.png" height="30">
							<h1 class="title">Radix Profile</h1>
						</a>
						<a class="" href="https://discord.gg/NH47Zxc8Re">
							<img class="img" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQo4yQ5O1kv2Uv1Y45cZQlZpvhgmx1oWW8nqg&usqp=CAU">
							<h1 class="title">Get in touch</h1>
						</a>
					</span>
				</section>
			</nav>

			<section class="container" id="main">
				<div class="panel" style="background-color: #1192fc;">
					Stake address: rv1qg0qqwmjun43knkyyhh5kavhljy6ml5ap5hnzr2ahqwyt7n0rqq56pehq5n
				</div>
				
					<div class="panel operational" data-l10n-id="systems-operational">
						All Systems Operational
					</div>
				

				<h4 data-l10n-id="systems">Systems</h4>
				<ul class="systems">
					
					<li>
						Monitoring system <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Olympia Backup Node <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Olympia Validator <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
					<li>
						Radix Stokenet <span class="status operational" data-l10n-id="operational">operational</span>
					</li>
					
				</ul>

				<h4 data-l10n-id="incidents">Incidents</h4>
				
					
					<div class="incident">
						<span class="date moment">2022-01-24 13:48:17 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Node uptime down to 99.8%</span>
						<p>Our node is misbehaving since mid January. We see quite a few peer timeouts, the team is looking at it.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/150794329-0ec4bc0a-6dc8-4f54-b13d-cab4dcb2abc0.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2022-01-26 19:02:59 UTC</span></em></p>
							<p>Cleaned up address and switched to backup node</p>

						
							<p><em>Update <span class="moment">2022-01-26 19:11:16 UTC</span></em></p>
							<p>Our two nodes have been competing for some minutes for the proposals which dropped our uptime:</p>

<p><img src="https://user-images.githubusercontent.com/1419573/151230376-f1b06dd2-85cd-4e95-a858-abf9607489fa.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2022-01-26 19:21:17 UTC</span></em></p>
							<p>These are the metrics of our backup node:</p>

<p><img src="https://user-images.githubusercontent.com/1419573/151231562-15ac19aa-900f-4daf-bd03-66e81b1dc8f2.png" alt="image" /></p>

<p>and these are coming from our main node:</p>

<p><img src="https://user-images.githubusercontent.com/1419573/151231740-1f527785-2248-4ebd-812d-82c2134fb357.png" alt="image" /></p>

<p>The scaledown policy of the main node didn't go to 0 hence the nodes were failing wich leaded to node restart and additional downtime.</p>

						
					</div>
					
					<div class="incident">
						<span class="date moment">2021-12-10 08:13:00 UTC</span>

						
							<span class="label operational float-right" data-l10n-id="resolved">resolved</span>
						
						
							<span class="label system float-right">Radix Olympia Validator</span>
						
						<hr/>

						<span class="title">Down to 99.2%</span>
						<p>During the night we got some more missed proposals and didn't recover our uptime. We are looking at our options right now</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:32:28 UTC</span></em></p>
							<p>Switched to Backup node while we are cleaning up the address book of the main node. We suspect that our address book is way out of sync due to peer connection timeouts in our logs.</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:41:12 UTC</span></em></p>
							<p>Cleaned up Main node address book and switched back to it. Backup node up and running again as well</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:43:42 UTC</span></em></p>
							<p>Node is stable again, we operated during 6 minutes with our backup node while we cleaned up the address of the main one.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145600999-3e0dd3a1-e72d-4094-9c8d-cefab3f000b2.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-10 15:44:56 UTC</span></em></p>
							<p>Our alerting was quite unstable lately, we expect the issue to be gone after the Address book cleanup which removed around 530 addresses that aren't in use anymore. You can see our alerts in the last 7 days below:</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145601245-696a6cb2-c049-4dbc-941d-4453b637de6e.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-10 15:46:25 UTC</span></em></p>
							<p>No downtime during the nodes switches within our Kubernetes cluster now that our terraform resources are back up and running and we can do the switches as usual. We will be monitoring closely the situation</p>

						
							<p><em>Update <span class="moment">2021-12-10 15:51:58 UTC</span></em></p>
							<p>We saw no network issues nor CPU/RAM/Provider downtime during the high alerting period.</p>

<p><img src="https://user-images.githubusercontent.com/1419573/145602067-b2100b00-75e8-44c0-b1e8-8c6b2490417b.png" alt="image" />
<img src="https://user-images.githubusercontent.com/1419573/145602258-9ea165de-1c15-4853-b6df-e069a45f8c5a.png" alt="image" />
<img src="https://user-images.githubusercontent.com/1419573/145602289-337de5ab-a81f-43f0-a82b-edcbe96985f7.png" alt="image" /></p>

						
							<p><em>Update <span class="moment">2021-12-11 14:40:30 UTC</span></em></p>
							<p>Our reverse proxy (NGINX) had lost it's connection to our main node and was not allowing connection which caused timeouts for our node. The issue has been fixed and we added further alarming to detect if such case happened again. We expect our uptime to recover from now on</p>

						
							<p><em>Update <span class="moment">2021-12-18 10:11:35 UTC</span></em></p>
							<p>Took us some time till the uptime recovered but we are back to normal now</p>

						
					</div>
					
				
			</section>


			<footer class="footer">
				<section class="container">
					<hr/>
					<p>If you experience any issue please contact us on Discord: https://discord.gg/NH47Zxc8Re</p>
				</section>
			</footer>

		</main>
		<script src="statuspage.js" type="text/javascript"></script>
	</body>
</html>